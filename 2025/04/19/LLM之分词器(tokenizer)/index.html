<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon_32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon_16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="LLM之分词器(tokenizer)一、引言大语言模型在自然语言处理领域取得了巨大成功，而分词器作为其关键组成部分，对模型的性能和效果有着显著影响。今天，我们将探讨四种常见的分词方法：BPE、WordPiece、SentencePiece 和 unigram，剖析它们的技术原理、实现细节及应用场景。 在此之前，我们需要了解一下什么是tokenizition。 任何一段文本，输入给模型，都是要转换成">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM之分词器(tokenizer)">
<meta property="og:url" content="http://example.com/2025/04/19/LLM%E4%B9%8B%E5%88%86%E8%AF%8D%E5%99%A8(tokenizer)/index.html">
<meta property="og:site_name" content="Blog of TheChef">
<meta property="og:description" content="LLM之分词器(tokenizer)一、引言大语言模型在自然语言处理领域取得了巨大成功，而分词器作为其关键组成部分，对模型的性能和效果有着显著影响。今天，我们将探讨四种常见的分词方法：BPE、WordPiece、SentencePiece 和 unigram，剖析它们的技术原理、实现细节及应用场景。 在此之前，我们需要了解一下什么是tokenizition。 任何一段文本，输入给模型，都是要转换成">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-04-19T07:28:31.000Z">
<meta property="article:modified_time" content="2025-04-19T07:55:09.112Z">
<meta property="article:author" content="TheChef">
<meta property="article:tag" content="llm">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2025/04/19/LLM%E4%B9%8B%E5%88%86%E8%AF%8D%E5%99%A8(tokenizer)/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>LLM之分词器(tokenizer) | Blog of TheChef</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Blog of TheChef" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Blog of TheChef</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/04/19/LLM%E4%B9%8B%E5%88%86%E8%AF%8D%E5%99%A8(tokenizer)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="TheChef">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog of TheChef">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          LLM之分词器(tokenizer)
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-04-19 15:28:31 / 修改时间：15:55:09" itemprop="dateCreated datePublished" datetime="2025-04-19T15:28:31+08:00">2025-04-19</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%9F%A5%E8%AF%86%E7%A7%AF%E7%B4%AF/" itemprop="url" rel="index"><span itemprop="name">知识积累</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="LLM之分词器-tokenizer"><a href="#LLM之分词器-tokenizer" class="headerlink" title="LLM之分词器(tokenizer)"></a>LLM之分词器(tokenizer)</h1><h2 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h2><p>大语言模型在自然语言处理领域取得了巨大成功，而分词器作为其关键组成部分，对模型的性能和效果有着显著影响。今天，我们将探讨四种常见的分词方法：BPE、WordPiece、SentencePiece 和 unigram，剖析它们的技术原理、实现细节及应用场景。</p>
<p>在此之前，我们需要了解一下什么是tokenizition。</p>
<p>任何一段文本，输入给模型，都是要转换成一串embedding。这个过程简单概括为：</p>
<ol>
<li>分词，并把词转换为token（即词的ID）</li>
<li>token转换成embedding</li>
</ol>
<p>而tokenization就是在做这第一步。而对于第二步就是常见的Embedding查表操作，即根据token_id的值，去Embedding矩阵中查找第token_id行的数据作为embedding。</p>
<span id="more"></span>

<h2 id="二、方法介绍"><a href="#二、方法介绍" class="headerlink" title="二、方法介绍"></a>二、方法介绍</h2><h3 id="1-BPE"><a href="#1-BPE" class="headerlink" title="1. BPE"></a>1. BPE</h3><p>BPE 是一种基于频率的分词算法，最初用于机器翻译任务中的词汇扩展。其核心思想是通过不断合并文本中出现频率最高的字节对来构建词汇表。具体来说，首先将文本中的每个字符视为一个独立的 token，然后统计所有相邻字符对的频率，选择最频繁的字符对进行合并，并更新词汇表和文本表示，重复这一过程直到达到预设的词汇表大小。其实现方法如下</p>
<ol>
<li>初始化词汇表：以字符为粒度，将文本中的每个字符作为初始词汇表的元素。</li>
<li>统计字节对频率：遍历文本，统计所有相邻字符对的出现次数。</li>
<li>合并高频字节对：选择频率最高的字节对进行合并，生成新的 token，并更新词汇表和文本表示。</li>
<li>迭代更新：重复统计频率和合并操作，直到词汇表大小达到设定值。</li>
</ol>
<p>Byte-Pair Encoding(BPE)是最广泛采用的subword分词器。</p>
<ul>
<li>训练方法：从字符级的小词表出发，训练产生合并规则以及一个词表</li>
<li>编码方法：将文本切分成字符，再应用训练阶段获得的合并规则</li>
<li>经典模型：GPT, GPT-2, RoBERTa, BART, LLaMA, ChatGLM等</li>
</ul>
<h4 id="1-1-训练阶段"><a href="#1-1-训练阶段" class="headerlink" title="1.1. 训练阶段"></a>1.1. 训练阶段</h4><p>在训练环节，目标是给定语料，通过训练算法，生成合并规则和词表。 BPE算法是从一个字符级别的词表为基础，合并pair并添加到词表中，逐步形成大词表。合并规则为选择相邻pair词频最大的进行合并。</p>
<p>假定训练的语料(已归一化处理)为4个句子。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">corpus = [</span><br><span class="line">    &quot;This is the Hugging Face Course.&quot;,</span><br><span class="line">    &quot;This chapter is about tokenization.&quot;,</span><br><span class="line">    &quot;This section shows several tokenizer algorithms.&quot;,</span><br><span class="line">    &quot;Hopefully, you will be able to understand how they are trained and generate tokens.&quot;,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>首先进行预切分处理。这里采用gpt2的预切分逻辑。 具体会按照空格和标点进行切分，并且空格会保留成特殊的字符“Ġ”。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from transformers import AutoTokenizer</span><br><span class="line"></span><br><span class="line"># init pre tokenize function</span><br><span class="line">gpt2_tokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)</span><br><span class="line">pre_tokenize_function = gpt2_tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str</span><br><span class="line"></span><br><span class="line"># pre tokenize</span><br><span class="line">pre_tokenized_corpus = [pre_tokenize_str(text) for text in corpus]</span><br></pre></td></tr></table></figure>

<p>获得的pre_tokenized_corpus如下，每个单元分别为[word, (start_index, end_index)]</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    [(&#x27;This&#x27;, (0, 4)), (&#x27;Ġis&#x27;, (4, 7)), (&#x27;Ġthe&#x27;, (7, 11)), (&#x27;ĠHugging&#x27;, (11, 19)), (&#x27;ĠFace&#x27;, (19, 24)), (&#x27;ĠCourse&#x27;, (24, 31)), (&#x27;.&#x27;, (31, 32))], </span><br><span class="line">    [(&#x27;This&#x27;, (0, 4)), (&#x27;Ġchapter&#x27;, (4, 12)), (&#x27;Ġis&#x27;, (12, 15)), (&#x27;Ġabout&#x27;, (15, 21)), (&#x27;Ġtokenization&#x27;, (21, 34)), (&#x27;.&#x27;, (34, 35))], </span><br><span class="line">    [(&#x27;This&#x27;, (0, 4)), (&#x27;Ġsection&#x27;, (4, 12)), (&#x27;Ġshows&#x27;, (12, 18)), (&#x27;Ġseveral&#x27;, (18, 26)), (&#x27;Ġtokenizer&#x27;, (26, 36)), (&#x27;Ġalgorithms&#x27;, (36, 47)), (&#x27;.&#x27;, (47, 48))], </span><br><span class="line">    [(&#x27;Hopefully&#x27;, (0, 9)), (&#x27;,&#x27;, (9, 10)), (&#x27;Ġyou&#x27;, (10, 14)), (&#x27;Ġwill&#x27;, (14, 19)), (&#x27;Ġbe&#x27;, (19, 22)), (&#x27;Ġable&#x27;, (22, 27)), (&#x27;Ġto&#x27;, (27, 30)), (&#x27;Ġunderstand&#x27;, (30, 41)), (&#x27;Ġhow&#x27;, (41, 45)), (&#x27;Ġthey&#x27;, (45, 50)), (&#x27;Ġare&#x27;, (50, 54)), (&#x27;Ġtrained&#x27;, (54, 62)), (&#x27;Ġand&#x27;, (62, 66)), (&#x27;Ġgenerate&#x27;, (66, 75)), (&#x27;Ġtokens&#x27;, (75, 82)), (&#x27;.&#x27;, (82, 83))]</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>进一步统计每个整词的词频</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">word2count = defaultdict(int)</span><br><span class="line">for split_text in pre_tokenized_corpus:</span><br><span class="line">    for word, _ in split_text:</span><br><span class="line">        word2count[word] += 1</span><br></pre></td></tr></table></figure>

<p>获得word2count如下</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">defaultdict(&lt;class &#x27;int&#x27;&gt;, &#123;&#x27;This&#x27;: 3, &#x27;Ġis&#x27;: 2, &#x27;Ġthe&#x27;: 1, &#x27;ĠHugging&#x27;: 1, &#x27;ĠFace&#x27;: 1, &#x27;ĠCourse&#x27;: 1, &#x27;.&#x27;: 4, &#x27;Ġchapter&#x27;: 1, &#x27;Ġabout&#x27;: 1, &#x27;Ġtokenization&#x27;: 1, &#x27;Ġsection&#x27;: 1, &#x27;Ġshows&#x27;: 1, &#x27;Ġseveral&#x27;: 1, &#x27;Ġtokenizer&#x27;: 1, &#x27;Ġalgorithms&#x27;: 1, &#x27;Hopefully&#x27;: 1, &#x27;,&#x27;: 1, &#x27;Ġyou&#x27;: 1, &#x27;Ġwill&#x27;: 1, &#x27;Ġbe&#x27;: 1, &#x27;Ġable&#x27;: 1, &#x27;Ġto&#x27;: 1, &#x27;Ġunderstand&#x27;: 1, &#x27;Ġhow&#x27;: 1, &#x27;Ġthey&#x27;: 1, &#x27;Ġare&#x27;: 1, &#x27;Ġtrained&#x27;: 1, &#x27;Ġand&#x27;: 1, &#x27;Ġgenerate&#x27;: 1, &#x27;Ġtokens&#x27;: 1&#125;)</span><br></pre></td></tr></table></figure>

<p>因为BPE是从字符级别的小词表，逐步合并成大词表，所以需要先获得字符级别的小词表。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vocab_set = set()</span><br><span class="line">for word in word2count:</span><br><span class="line">    vocab_set.update(list(word))</span><br><span class="line">vocabs = list(vocab_set)</span><br></pre></td></tr></table></figure>

<p>获得的初始小词表vocabs如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;i&#x27;, &#x27;t&#x27;, &#x27;p&#x27;, &#x27;o&#x27;, &#x27;r&#x27;, &#x27;m&#x27;, &#x27;e&#x27;, &#x27;,&#x27;, &#x27;y&#x27;, &#x27;v&#x27;, &#x27;Ġ&#x27;, &#x27;F&#x27;, &#x27;a&#x27;, &#x27;C&#x27;, &#x27;H&#x27;, &#x27;.&#x27;, &#x27;f&#x27;, &#x27;l&#x27;, &#x27;u&#x27;, &#x27;c&#x27;, &#x27;T&#x27;, &#x27;k&#x27;, &#x27;h&#x27;, &#x27;z&#x27;, &#x27;d&#x27;, &#x27;g&#x27;, &#x27;w&#x27;, &#x27;n&#x27;, &#x27;s&#x27;, &#x27;b&#x27;]</span><br></pre></td></tr></table></figure>

<p>基于小词表就可以对每个整词进行切分</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">word2splits = &#123;word: [c for c in word] for word in word2count&#125;</span><br><span class="line"></span><br><span class="line">&#x27;This&#x27;: [&#x27;T&#x27;, &#x27;h&#x27;, &#x27;i&#x27;, &#x27;s&#x27;], </span><br><span class="line">&#x27;Ġis&#x27;: [&#x27;Ġ&#x27;, &#x27;i&#x27;, &#x27;s&#x27;], </span><br><span class="line">&#x27;Ġthe&#x27;: [&#x27;Ġ&#x27;, &#x27;t&#x27;, &#x27;h&#x27;, &#x27;e&#x27;], </span><br><span class="line">...</span><br><span class="line">&#x27;Ġand&#x27;: [&#x27;Ġ&#x27;, &#x27;a&#x27;, &#x27;n&#x27;, &#x27;d&#x27;], </span><br><span class="line">&#x27;Ġgenerate&#x27;: [&#x27;Ġ&#x27;, &#x27;g&#x27;, &#x27;e&#x27;, &#x27;n&#x27;, &#x27;e&#x27;, &#x27;r&#x27;, &#x27;a&#x27;, &#x27;t&#x27;, &#x27;e&#x27;], </span><br><span class="line">&#x27;Ġtokens&#x27;: [&#x27;Ġ&#x27;, &#x27;t&#x27;, &#x27;o&#x27;, &#x27;k&#x27;, &#x27;e&#x27;, &#x27;n&#x27;, &#x27;s&#x27;]</span><br></pre></td></tr></table></figure>

<p>基于word2splits统计vocabs中相邻两个pair的词频pair2count</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def _compute_pair2score(word2splits, word2count):</span><br><span class="line">    pair2count = defaultdict(int)</span><br><span class="line">    for word, word_count in word2count.items():</span><br><span class="line">        split = word2splits[word]</span><br><span class="line">        if len(split) == 1:</span><br><span class="line">            continue</span><br><span class="line">        for i in range(len(split) - 1):</span><br><span class="line">            pair = (split[i], split[i + 1])</span><br><span class="line">            pair2count[pair] += word_count</span><br><span class="line">    return pair2count</span><br></pre></td></tr></table></figure>

<p>获得pair2count如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">defaultdict(&lt;class &#x27;int&#x27;&gt;, &#123;(&#x27;T&#x27;, &#x27;h&#x27;): 3, (&#x27;h&#x27;, &#x27;i&#x27;): 3, (&#x27;i&#x27;, &#x27;s&#x27;): 5, (&#x27;Ġ&#x27;, &#x27;i&#x27;): 2, (&#x27;Ġ&#x27;, &#x27;t&#x27;): 7, (&#x27;t&#x27;, &#x27;h&#x27;): 3, ..., (&#x27;n&#x27;, &#x27;s&#x27;): 1&#125;)</span><br></pre></td></tr></table></figure>

<p>统计当前频率最高的相邻pair</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def _compute_most_score_pair(pair2count):</span><br><span class="line">    best_pair = None</span><br><span class="line">    max_freq = None</span><br><span class="line">    for pair, freq in pair2count.items():</span><br><span class="line">        if max_freq is None or max_freq &lt; freq:</span><br><span class="line">            best_pair = pair</span><br><span class="line">            max_freq = freq</span><br><span class="line">    return best_pair</span><br></pre></td></tr></table></figure>

<p>经过统计，当前频率最高的pair为: (‘Ġ’, ‘t’)， 频率为7次。 将(‘Ġ’, ‘t’)合并成一个词并添加到词表中。同时在合并规则中添加(‘Ġ’, ‘t’)这条合并规则。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">merge_rules = []</span><br><span class="line">best_pair = self._compute_most_score_pair(pair2score)</span><br><span class="line">vocabs.append(best_pair[0] + best_pair[1])</span><br><span class="line">merge_rules.append(best_pair)</span><br></pre></td></tr></table></figure>

<p>此时的vocab词表更新成:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;i&#x27;, &#x27;t&#x27;, &#x27;p&#x27;, &#x27;o&#x27;, &#x27;r&#x27;, &#x27;m&#x27;, &#x27;e&#x27;, &#x27;,&#x27;, &#x27;y&#x27;, &#x27;v&#x27;, &#x27;Ġ&#x27;, &#x27;F&#x27;, &#x27;a&#x27;, &#x27;C&#x27;, &#x27;H&#x27;, &#x27;.&#x27;, &#x27;f&#x27;, &#x27;l&#x27;, &#x27;u&#x27;, &#x27;c&#x27;, &#x27;T&#x27;, &#x27;k&#x27;, &#x27;h&#x27;, &#x27;z&#x27;, &#x27;d&#x27;, &#x27;g&#x27;, &#x27;w&#x27;, &#x27;n&#x27;, &#x27;s&#x27;, &#x27;b&#x27;, </span><br><span class="line">&#x27;Ġt&#x27;]</span><br></pre></td></tr></table></figure>

<p>根据更新后的vocab重新对word2count进行切分。具体实现上，可以直接在旧的word2split上应用新的合并规则(‘Ġ’, ‘t’)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def _merge_pair(a, b, word2splits):</span><br><span class="line">    new_word2splits = dict()</span><br><span class="line">    for word, split in word2splits.items():</span><br><span class="line">        if len(split) == 1:</span><br><span class="line">            new_word2splits[word] = split</span><br><span class="line">            continue</span><br><span class="line">        i = 0</span><br><span class="line">        while i &lt; len(split) - 1:</span><br><span class="line">            if split[i] == a and split[i + 1] == b:</span><br><span class="line">                split = split[:i] + [a + b] + split[i + 2:]</span><br><span class="line">            else:</span><br><span class="line">                i += 1</span><br><span class="line">        new_word2splits[word] = split</span><br><span class="line">    return new_word2splits</span><br></pre></td></tr></table></figure>

<p>从而获得新的word2split</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;This&#x27;: [&#x27;T&#x27;, &#x27;h&#x27;, &#x27;i&#x27;, &#x27;s&#x27;], </span><br><span class="line">&#x27;Ġis&#x27;: [&#x27;Ġ&#x27;, &#x27;i&#x27;, &#x27;s&#x27;], </span><br><span class="line">&#x27;Ġthe&#x27;: [&#x27;Ġt&#x27;, &#x27;h&#x27;, &#x27;e&#x27;], </span><br><span class="line">&#x27;ĠHugging&#x27;: [&#x27;Ġ&#x27;, &#x27;H&#x27;, &#x27;u&#x27;, &#x27;g&#x27;, &#x27;g&#x27;, &#x27;i&#x27;, &#x27;n&#x27;, &#x27;g&#x27;],</span><br><span class="line">...</span><br><span class="line">&#x27;Ġtokens&#x27;: [&#x27;Ġt&#x27;, &#x27;o&#x27;, &#x27;k&#x27;, &#x27;e&#x27;, &#x27;n&#x27;, &#x27;s&#x27;]&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到新的word2split中已经包含了新的词”Ġt”。</p>
<p>重复上述循环直到整个词表的大小达到预先设定的词表大小。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">while len(vocabs) &lt; vocab_size:</span><br><span class="line">    pair2score = self._compute_pair2score(word2splits, word2count)</span><br><span class="line">    best_pair = self._compute_most_score_pair(pair2score)</span><br><span class="line">    vocabs.append(best_pair[0] + best_pair[1])</span><br><span class="line">    merge_rules.append(best_pair)</span><br><span class="line">    word2splits = self._merge_pair(best_pair[0], best_pair[1], word2splits)</span><br></pre></td></tr></table></figure>

<p>假定最终词表的大小为50，经过上述迭代后我们获得的词表和合并规则如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vocabs = [&#x27;i&#x27;, &#x27;t&#x27;, &#x27;p&#x27;, &#x27;o&#x27;, &#x27;r&#x27;, &#x27;m&#x27;, &#x27;e&#x27;, &#x27;,&#x27;, &#x27;y&#x27;, &#x27;v&#x27;, &#x27;Ġ&#x27;, &#x27;F&#x27;, &#x27;a&#x27;, &#x27;C&#x27;, &#x27;H&#x27;, &#x27;.&#x27;, &#x27;f&#x27;, &#x27;l&#x27;, &#x27;u&#x27;, &#x27;c&#x27;, &#x27;T&#x27;, &#x27;k&#x27;, &#x27;h&#x27;, &#x27;z&#x27;, &#x27;d&#x27;, &#x27;g&#x27;, &#x27;w&#x27;, &#x27;n&#x27;, &#x27;s&#x27;, &#x27;b&#x27;, &#x27;Ġt&#x27;, &#x27;is&#x27;, &#x27;er&#x27;, &#x27;Ġa&#x27;, &#x27;Ġto&#x27;, &#x27;en&#x27;, &#x27;Th&#x27;, &#x27;This&#x27;, &#x27;ou&#x27;, &#x27;se&#x27;, &#x27;Ġtok&#x27;, &#x27;Ġtoken&#x27;, &#x27;nd&#x27;, &#x27;Ġis&#x27;, &#x27;Ġth&#x27;, &#x27;Ġthe&#x27;, &#x27;in&#x27;, &#x27;Ġab&#x27;, &#x27;Ġtokeni&#x27;, &#x27;Ġtokeniz&#x27;]</span><br><span class="line"></span><br><span class="line">merge_rules = [(&#x27;Ġ&#x27;, &#x27;t&#x27;), (&#x27;i&#x27;, &#x27;s&#x27;), (&#x27;e&#x27;, &#x27;r&#x27;), (&#x27;Ġ&#x27;, &#x27;a&#x27;), (&#x27;Ġt&#x27;, &#x27;o&#x27;), (&#x27;e&#x27;, &#x27;n&#x27;), (&#x27;T&#x27;, &#x27;h&#x27;), (&#x27;Th&#x27;, &#x27;is&#x27;), (&#x27;o&#x27;, &#x27;u&#x27;), (&#x27;s&#x27;, &#x27;e&#x27;), (&#x27;Ġto&#x27;, &#x27;k&#x27;), (&#x27;Ġtok&#x27;, &#x27;en&#x27;), (&#x27;n&#x27;, &#x27;d&#x27;), (&#x27;Ġ&#x27;, &#x27;is&#x27;), (&#x27;Ġt&#x27;, &#x27;h&#x27;), (&#x27;Ġth&#x27;, &#x27;e&#x27;), (&#x27;i&#x27;, &#x27;n&#x27;), (&#x27;Ġa&#x27;, &#x27;b&#x27;), (&#x27;Ġtoken&#x27;, &#x27;i&#x27;), (&#x27;Ġtokeni&#x27;, &#x27;z&#x27;)]</span><br></pre></td></tr></table></figure>

<p>至此我们就根据给定的语料完成了BPE分词器的训练。</p>
<h4 id="1-2-推理阶段"><a href="#1-2-推理阶段" class="headerlink" title="1.2. 推理阶段"></a>1.2. 推理阶段</h4><p>在推理阶段，给定一个句子，我们需要将其切分成一个token的序列。 具体实现上需要先对句子进行预分词并切分成字符级别的序列，然后根据合并规则进行合并。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def tokenize(self, text: str) -&gt; List[str]:</span><br><span class="line">    # pre tokenize</span><br><span class="line">    words = [word for word, _ in self.pre_tokenize_str(text)]</span><br><span class="line">    # split into char level</span><br><span class="line">    splits = [[c for c in word] for word in words]</span><br><span class="line">    # apply merge rules</span><br><span class="line">    for merge_rule in self.merge_rules:</span><br><span class="line">        for index, split in enumerate(splits):</span><br><span class="line">            i = 0</span><br><span class="line">            while i &lt; len(split) - 1:</span><br><span class="line">                if split[i] == merge_rule[0] and split[i + 1] == merge_rule[1]:</span><br><span class="line">                    split = split[:i] + [&quot;&quot;.join(merge_rule)] + split[i + 2:]</span><br><span class="line">                else:</span><br><span class="line">                    i += 1</span><br><span class="line">            splits[index] = split</span><br><span class="line">    return sum(splits, [])</span><br></pre></td></tr></table></figure>

<p>例如</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; tokenize(&quot;This is not a token.&quot;)</span><br><span class="line">&gt;&gt;&gt; [&#x27;This&#x27;, &#x27;Ġis&#x27;, &#x27;Ġ&#x27;, &#x27;n&#x27;, &#x27;o&#x27;, &#x27;t&#x27;, &#x27;Ġa&#x27;, &#x27;Ġtoken&#x27;, &#x27;.&#x27;]</span><br></pre></td></tr></table></figure>

<h3 id="2-WordPiece"><a href="#2-WordPiece" class="headerlink" title="2. WordPiece"></a>2. WordPiece</h3><p>WordPiece 也是一种基于频率的分词方法，与 BPE 不同的是，它在选择合并单元时，不仅考虑字节对的出现频率，还引入了语言模型的似然估计。其目标是最小化语言模型的困惑度，即选择使语言模型概率最大的子词划分方式。[只是在训练阶段合并pair的策略不是pair的频率而是互信息。]</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">socre=log(p(ab))−(log(p(a))+log(p(b)))=log(p(ab)/p(a)p(b))</span><br></pre></td></tr></table></figure>



<p>这里的动机是一个pair的频率很高，但是其中pair的一部分的频率更高，这时候不一定需要进行该pair的合并。 而如果一个pair的频率很高，并且这个pair的两个部分都是只出现在这个pair中，就说明这个pair很值得合并。实现步骤如下：</p>
<ol>
<li>初始化词汇表：通常以字符为初始 token。</li>
<li>训练语言模型：使用初始词汇表对文本进行编码，并训练一个语言模型。</li>
<li>寻找最优合并：在每次迭代中，尝试所有可能的子词对合并，计算合并后的语言模型困惑度，选择使困惑度最小的合并对。</li>
<li>更新词汇表和语言模型：将新合并的子词加入词汇表，并重新训练语言模型。</li>
<li>重复迭代：直到达到预设的词汇表大小或困惑度不再显著降低。</li>
</ol>
<ul>
<li>训练方法：从字符级的小词表出发，训练产生合并规则以及一个词表</li>
<li>编码方法：将文本切分成词，对每个词在词表中进行最大前向匹配</li>
<li>经典模型：BERT及其系列DistilBERT，MobileBERT等</li>
</ul>
<h4 id="2-1-训练阶段"><a href="#2-1-训练阶段" class="headerlink" title="2.1. 训练阶段"></a>2.1. 训练阶段</h4><p>在训练环节，给定语料，通过训练算法，生成最终的词表。 WordPiece算法也是从一个字符级别的词表为基础，逐步扩充成大词表。合并规则为选择相邻pair互信息最大的进行合并。</p>
<p>下面进行具体手工实现。</p>
<p>假定训练的语料(已归一化处理)为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">corpus = [</span><br><span class="line">    &quot;This is the Hugging Face Course.&quot;,</span><br><span class="line">    &quot;This chapter is about tokenization.&quot;,</span><br><span class="line">    &quot;This section shows several tokenizer algorithms.&quot;,</span><br><span class="line">    &quot;Hopefully, you will be able to understand how they are trained and generate tokens.&quot;,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>首先进行预切分处理。这里采用BERT的预切分逻辑。具体会按照空格和标点进行切分。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from transformers import AutoTokenizer</span><br><span class="line"></span><br><span class="line"># init pre tokenize function</span><br><span class="line">bert_tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)</span><br><span class="line">pre_tokenize_function = bert_tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str</span><br><span class="line"></span><br><span class="line"># pre tokenize</span><br><span class="line">pre_tokenized_corpus = [pre_tokenize_str(text) for text in corpus]</span><br></pre></td></tr></table></figure>

<p>获得的pre_tokenized_corpus如下，每个单元分别为[word, (start_index, end_index)]</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    [(&#x27;This&#x27;, (0, 4)), (&#x27;is&#x27;, (5, 7)), (&#x27;the&#x27;, (8, 11)), (&#x27;Hugging&#x27;, (12, 19)), (&#x27;Face&#x27;, (20, 24)), (&#x27;Course&#x27;, (25, 31)), (&#x27;.&#x27;, (31, 32))], </span><br><span class="line">    [(&#x27;This&#x27;, (0, 4)), (&#x27;chapter&#x27;, (5, 12)), (&#x27;is&#x27;, (13, 15)), (&#x27;about&#x27;, (16, 21)), (&#x27;tokenization&#x27;, (22, 34)), (&#x27;.&#x27;, (34, 35))], </span><br><span class="line">    [(&#x27;This&#x27;, (0, 4)), (&#x27;section&#x27;, (5, 12)), (&#x27;shows&#x27;, (13, 18)), (&#x27;several&#x27;, (19, 26)), (&#x27;tokenizer&#x27;, (27, 36)), (&#x27;algorithms&#x27;, (37, 47)), (&#x27;.&#x27;, (47, 48))], </span><br><span class="line">    [(&#x27;Hopefully&#x27;, (0, 9)), (&#x27;,&#x27;, (9, 10)), (&#x27;you&#x27;, (11, 14)), (&#x27;will&#x27;, (15, 19)), (&#x27;be&#x27;, (20, 22)), (&#x27;able&#x27;, (23, 27)), (&#x27;to&#x27;, (28, 30)), (&#x27;understand&#x27;, (31, 41)), (&#x27;how&#x27;, (42, 45)), (&#x27;they&#x27;, (46, 50)), (&#x27;are&#x27;, (51, 54)), (&#x27;trained&#x27;, (55, 62)), (&#x27;and&#x27;, (63, 66)), (&#x27;generate&#x27;, (67, 75)), (&#x27;tokens&#x27;, (76, 82)), (&#x27;.&#x27;, (82, 83))]</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>进一步统计词频</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">word2count = defaultdict(int)</span><br><span class="line">for split_text in pre_tokenized_corpus:</span><br><span class="line">    for word, _ in split_text:</span><br><span class="line">        word2count[word] += 1</span><br></pre></td></tr></table></figure>

<p>获得word2count如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">defaultdict(&lt;class &#x27;int&#x27;&gt;, &#123;&#x27;This&#x27;: 3, &#x27;is&#x27;: 2, &#x27;the&#x27;: 1, &#x27;Hugging&#x27;: 1, &#x27;Face&#x27;: 1, &#x27;Course&#x27;: 1, &#x27;.&#x27;: 4, &#x27;chapter&#x27;: 1, &#x27;about&#x27;: 1, &#x27;tokenization&#x27;: 1, &#x27;section&#x27;: 1, &#x27;shows&#x27;: 1, &#x27;several&#x27;: 1, &#x27;tokenizer&#x27;: 1, &#x27;algorithms&#x27;: 1, &#x27;Hopefully&#x27;: 1, &#x27;,&#x27;: 1, &#x27;you&#x27;: 1, &#x27;will&#x27;: 1, &#x27;be&#x27;: 1, &#x27;able&#x27;: 1, &#x27;to&#x27;: 1, &#x27;understand&#x27;: 1, &#x27;how&#x27;: 1, &#x27;they&#x27;: 1, &#x27;are&#x27;: 1, &#x27;trained&#x27;: 1, &#x27;and&#x27;: 1, &#x27;generate&#x27;: 1, &#x27;tokens&#x27;: 1&#125;)</span><br></pre></td></tr></table></figure>

<p>因为WordPiece同样是从字符级别的小词表，逐步合并成大词表，所以先获得字符级别的小词表。注意这里如果字符不是不一个词的开始，需要添加上特殊字符”##”。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vocab_set = set()</span><br><span class="line">for word in word2count:</span><br><span class="line">    vocab_set.add(word[0])</span><br><span class="line">    vocab_set.update([&#x27;##&#x27; + c for c in word[1:]])</span><br><span class="line">vocabs = list(vocab_set)</span><br></pre></td></tr></table></figure>

<p>获得的初始小词表vocabs如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;##a&#x27;, &#x27;##b&#x27;, &#x27;##c&#x27;, &#x27;##d&#x27;, &#x27;##e&#x27;, &#x27;##f&#x27;, &#x27;##g&#x27;, &#x27;##h&#x27;, &#x27;##i&#x27;, &#x27;##k&#x27;, &#x27;##l&#x27;, &#x27;##m&#x27;, &#x27;##n&#x27;, &#x27;##o&#x27;, &#x27;##p&#x27;, &#x27;##r&#x27;, &#x27;##s&#x27;, &#x27;##t&#x27;, &#x27;##u&#x27;, &#x27;##v&#x27;, &#x27;##w&#x27;, &#x27;##y&#x27;, &#x27;##z&#x27;, &#x27;,&#x27;, &#x27;.&#x27;, &#x27;C&#x27;, &#x27;F&#x27;, &#x27;H&#x27;, &#x27;T&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;g&#x27;, &#x27;h&#x27;, &#x27;i&#x27;, &#x27;s&#x27;, &#x27;t&#x27;, &#x27;u&#x27;, &#x27;w&#x27;, &#x27;y&#x27;]</span><br></pre></td></tr></table></figure>

<p>基于小词表对每个词进行切分</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">word2splits = &#123;word: [word[0]] + [&#x27;##&#x27; + c for c in word[1:]] for word in word2count&#125;</span><br><span class="line"></span><br><span class="line">&#123;&#x27;This&#x27;: [&#x27;T&#x27;, &#x27;##h&#x27;, &#x27;##i&#x27;, &#x27;##s&#x27;], </span><br><span class="line">&#x27;is&#x27;: [&#x27;i&#x27;, &#x27;##s&#x27;], </span><br><span class="line">&#x27;the&#x27;: [&#x27;t&#x27;, &#x27;##h&#x27;, &#x27;##e&#x27;], </span><br><span class="line">&#x27;Hugging&#x27;: [&#x27;H&#x27;, &#x27;##u&#x27;, &#x27;##g&#x27;, &#x27;##g&#x27;, &#x27;##i&#x27;, &#x27;##n&#x27;, &#x27;##g&#x27;], </span><br><span class="line">...</span><br><span class="line">&#x27;generate&#x27;: [&#x27;g&#x27;, &#x27;##e&#x27;, &#x27;##n&#x27;, &#x27;##e&#x27;, &#x27;##r&#x27;, &#x27;##a&#x27;, &#x27;##t&#x27;, &#x27;##e&#x27;], </span><br><span class="line">&#x27;tokens&#x27;: [&#x27;t&#x27;, &#x27;##o&#x27;, &#x27;##k&#x27;, &#x27;##e&#x27;, &#x27;##n&#x27;, &#x27;##s&#x27;]&#125;</span><br></pre></td></tr></table></figure>

<p>进一步统计vocabs中相邻两个pair的互信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def _compute_pair2score(word2splits, word2count):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    计算每个pair的分数</span><br><span class="line">    score=(freq_of_pair)/(freq_of_first_element×freq_of_second_element)</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    vocab2count = defaultdict(int)</span><br><span class="line">    pair2count = defaultdict(int)</span><br><span class="line">    for word, word_count in word2count.items():</span><br><span class="line">        splits = word2splits[word]</span><br><span class="line">        if len(splits) == 1:</span><br><span class="line">            vocab2count[splits[0]] += word_count</span><br><span class="line">            continue</span><br><span class="line">        for i in range(len(splits) - 1):</span><br><span class="line">            pair = (splits[i], splits[i + 1])</span><br><span class="line">            vocab2count[splits[i]] += word_count</span><br><span class="line">            pair2count[pair] += word_count</span><br><span class="line">        vocab2count[splits[-1]] += word_count</span><br><span class="line">    scores = &#123;</span><br><span class="line">        pair: freq / (vocab2count[pair[0]] * vocab2count[pair[1]])</span><br><span class="line">        for pair, freq in pair2count.items()</span><br><span class="line">    &#125;</span><br><span class="line">    return scores</span><br></pre></td></tr></table></figure>

<p>获得每个pair的互信息如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;(&#x27;T&#x27;, &#x27;##h&#x27;): 0.125, </span><br><span class="line">(&#x27;##h&#x27;, &#x27;##i&#x27;): 0.03409090909090909, </span><br><span class="line">(&#x27;##i&#x27;, &#x27;##s&#x27;): 0.02727272727272727, </span><br><span class="line">(&#x27;a&#x27;, &#x27;##b&#x27;): 0.2,</span><br><span class="line">...</span><br><span class="line">(&#x27;##n&#x27;, &#x27;##s&#x27;): 0.00909090909090909&#125;</span><br></pre></td></tr></table></figure>

<p>统计出互信息最高的相邻pair</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def _compute_most_score_pair(pair2score):</span><br><span class="line">    best_pair = None</span><br><span class="line">    max_score = None</span><br><span class="line">    for pair, score in pair2score.items():</span><br><span class="line">        if max_score is None or max_score &lt; score:</span><br><span class="line">            best_pair = pair</span><br><span class="line">            max_score = score</span><br><span class="line">    return best_pair</span><br></pre></td></tr></table></figure>

<p>此时互信息最高的pair为: (‘a’, ‘##b’) 将(‘a’, ‘##b’)合并成一个词’ab’并添加到词表中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">best_pair = self._compute_most_score_pair(pair2score)</span><br><span class="line">vocabs.append(best_pair[0] + best_pair[1])</span><br></pre></td></tr></table></figure>

<p>这样vocab词表更新成:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;##a&#x27;, &#x27;##b&#x27;, &#x27;##c&#x27;, &#x27;##d&#x27;, &#x27;##e&#x27;, &#x27;##f&#x27;, &#x27;##g&#x27;, &#x27;##h&#x27;, &#x27;##i&#x27;, &#x27;##k&#x27;, &#x27;##l&#x27;, &#x27;##m&#x27;, &#x27;##n&#x27;, &#x27;##o&#x27;, &#x27;##p&#x27;, &#x27;##r&#x27;, &#x27;##s&#x27;, &#x27;##t&#x27;, &#x27;##u&#x27;, &#x27;##v&#x27;, &#x27;##w&#x27;, &#x27;##y&#x27;, &#x27;##z&#x27;, &#x27;,&#x27;, &#x27;.&#x27;, &#x27;C&#x27;, &#x27;F&#x27;, &#x27;H&#x27;, &#x27;T&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;g&#x27;, &#x27;h&#x27;, &#x27;i&#x27;, &#x27;s&#x27;, &#x27;t&#x27;, &#x27;u&#x27;, &#x27;w&#x27;, &#x27;y&#x27;, </span><br><span class="line">&#x27;ab&#x27;]</span><br></pre></td></tr></table></figure>

<p>根据更新的vocab重新对word2count进行切分。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def _merge_pair(a, b, word2splits):</span><br><span class="line">    new_word2splits = dict()</span><br><span class="line">    for word, split in word2splits.items():</span><br><span class="line">        if len(split) == 1:</span><br><span class="line">            new_word2splits[word] = split</span><br><span class="line">            continue</span><br><span class="line">        i = 0</span><br><span class="line">        while i &lt; len(split) - 1:</span><br><span class="line">            if split[i] == a and split[i + 1] == b:</span><br><span class="line">                merge = a + b[2:] if b.startswith(&quot;##&quot;) else a + b</span><br><span class="line">                split = split[:i] + [merge] + split[i + 2:]</span><br><span class="line">            else:</span><br><span class="line">                i += 1</span><br><span class="line">        new_word2splits[word] = split</span><br><span class="line">    return new_word2splits</span><br></pre></td></tr></table></figure>

<p>获得新的word2split</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;This&#x27;: [&#x27;T&#x27;, &#x27;##h&#x27;, &#x27;##i&#x27;, &#x27;##s&#x27;], </span><br><span class="line">&#x27;is&#x27;: [&#x27;i&#x27;, &#x27;##s&#x27;], &#x27;the&#x27;: [&#x27;t&#x27;, &#x27;##h&#x27;, &#x27;##e&#x27;], </span><br><span class="line">&#x27;Hugging&#x27;: [&#x27;H&#x27;, &#x27;##u&#x27;, &#x27;##g&#x27;, &#x27;##g&#x27;, &#x27;##i&#x27;, &#x27;##n&#x27;, &#x27;##g&#x27;], </span><br><span class="line">&#x27;about&#x27;: [&#x27;ab&#x27;, &#x27;##o&#x27;, &#x27;##u&#x27;, &#x27;##t&#x27;], </span><br><span class="line">&#x27;tokens&#x27;: [&#x27;t&#x27;, &#x27;##o&#x27;, &#x27;##k&#x27;, &#x27;##e&#x27;, &#x27;##n&#x27;, &#x27;##s&#x27;]&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到新的word2split中已经包含了新的词”ab”。</p>
<p>重复上述步骤，直到整个词表的大小达到预先设定的词表大小。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">while len(vocabs) &lt; vocab_size:</span><br><span class="line">    pair2score = self._compute_pair2score(word2splits, word2count)</span><br><span class="line">    best_pair = self._compute_most_score_pair(pair2score)</span><br><span class="line">    word2splits = self._merge_pair(best_pair[0], best_pair[1], word2splits)</span><br><span class="line">    new_token = best_pair[0] + best_pair[1][2:] if best_pair[1].startswith(&#x27;##&#x27;) else best_pair[1]</span><br><span class="line">    vocabs.append(new_token)</span><br></pre></td></tr></table></figure>

<p>假定最终词表的大小为70，经过上述迭代后我们获得的词表如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vocabs = [&#x27;##a&#x27;, &#x27;##b&#x27;, &#x27;##c&#x27;, &#x27;##ct&#x27;, &#x27;##d&#x27;, &#x27;##e&#x27;, &#x27;##f&#x27;, &#x27;##fu&#x27;, &#x27;##ful&#x27;, &#x27;##full&#x27;, &#x27;##fully&#x27;, &#x27;##g&#x27;, &#x27;##h&#x27;, &#x27;##hm&#x27;, &#x27;##i&#x27;, &#x27;##k&#x27;, &#x27;##l&#x27;, &#x27;##m&#x27;, &#x27;##n&#x27;, &#x27;##o&#x27;, &#x27;##p&#x27;, &#x27;##r&#x27;, &#x27;##s&#x27;, &#x27;##t&#x27;, &#x27;##thm&#x27;, &#x27;##thms&#x27;, &#x27;##u&#x27;, &#x27;##ut&#x27;, &#x27;##v&#x27;, &#x27;##w&#x27;, &#x27;##y&#x27;, &#x27;##z&#x27;, &#x27;##za&#x27;, &#x27;##zat&#x27;, &#x27;,&#x27;, &#x27;.&#x27;, &#x27;C&#x27;, &#x27;F&#x27;, &#x27;Fa&#x27;, &#x27;Fac&#x27;, &#x27;H&#x27;, &#x27;Hu&#x27;, &#x27;Hug&#x27;, &#x27;Hugg&#x27;, &#x27;T&#x27;, &#x27;Th&#x27;, &#x27;a&#x27;, &#x27;ab&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;ch&#x27;, &#x27;cha&#x27;, &#x27;chap&#x27;, &#x27;chapt&#x27;, &#x27;g&#x27;, &#x27;h&#x27;, &#x27;i&#x27;, &#x27;is&#x27;, &#x27;s&#x27;, &#x27;sh&#x27;, &#x27;t&#x27;, &#x27;th&#x27;, &#x27;u&#x27;, &#x27;w&#x27;, &#x27;y&#x27;, &#x27;[CLS]&#x27;, &#x27;[MASK]&#x27;, &#x27;[PAD]&#x27;, &#x27;[SEP]&#x27;, &#x27;[UNK]&#x27;]</span><br></pre></td></tr></table></figure>

<p>注意词表中添加了特殊的token：[CLS], [MASK], [PAD], [SEP], [UNK] 至此我们就根据给定的语料完成了WordPiece分词器的训练。</p>
<h4 id="2-2-推理阶段"><a href="#2-2-推理阶段" class="headerlink" title="2.2. 推理阶段"></a>2.2. 推理阶段</h4><p>在推理阶段，给定一个句子，需要将其切分成一个token的序列。 具体实现上需要先对句子进行预分词，然后对每个词进行在词表中进行最大前向的匹配。如果词表中不存在则为UNK。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def _encode_word(self, word):</span><br><span class="line">    tokens = []</span><br><span class="line">    while len(word) &gt; 0:</span><br><span class="line">        i = len(word)</span><br><span class="line">        while i &gt; 0 and word[:i] not in self.vocabs:</span><br><span class="line">            i -= 1</span><br><span class="line">        if i == 0:</span><br><span class="line">            return [&quot;[UNK]&quot;]</span><br><span class="line">        tokens.append(word[:i])</span><br><span class="line">        word = word[i:]</span><br><span class="line">        if len(word) &gt; 0:</span><br><span class="line">            word = f&quot;##&#123;word&#125;&quot;</span><br><span class="line">    return tokens</span><br><span class="line"></span><br><span class="line">def tokenize(self, text):</span><br><span class="line">    words = [word for word, _ in self.pre_tokenize_str(text)]</span><br><span class="line">    encoded_words = [self._encode_word(word) for word in words]</span><br><span class="line">    return sum(encoded_words, [])</span><br></pre></td></tr></table></figure>

<p>例如</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; tokenize(&quot;This is the Hugging Face course!&quot;)</span><br><span class="line">&gt;&gt;&gt; [&#x27;Th&#x27;, &#x27;##i&#x27;, &#x27;##s&#x27;, &#x27;is&#x27;, &#x27;th&#x27;, &#x27;##e&#x27;, &#x27;Hugg&#x27;, &#x27;##i&#x27;, &#x27;##n&#x27;, &#x27;##g&#x27;, &#x27;Fac&#x27;, &#x27;##e&#x27;, &#x27;c&#x27;, &#x27;##o&#x27;, &#x27;##u&#x27;, &#x27;##r&#x27;, &#x27;##s&#x27;, &#x27;##e&#x27;, &#x27;[UNK]&#x27;]</span><br></pre></td></tr></table></figure>

<h3 id="3-Unigram"><a href="#3-Unigram" class="headerlink" title="3. Unigram"></a>3. Unigram</h3><p>Unigram分词与BPE和WordPiece不同，是基于一个大词表逐步裁剪成一个小词表。 通过Unigram语言模型计算删除不同subword造成的损失来衡量subword的重要性，保留重要性较高的子词。</p>
<ul>
<li>训练方法：从包含字符和全部子词的大词表出发，逐步裁剪出一个小词表，并且每个词都有自己的分数。</li>
<li>编码方法：将文本切分成词，对每个词基于Viterbi算法求解出最佳解码路径。</li>
<li>经典模型：AlBERT, T5, mBART, Big Bird, XLNet</li>
</ul>
<h4 id="3-1-训练阶段"><a href="#3-1-训练阶段" class="headerlink" title="3.1. 训练阶段"></a>3.1. 训练阶段</h4><p>在训练环节，目标是给定语料，通过训练算法，生成最终的词表，并且每个词有自己的概率值。 Unigram算法是从大词表为基础，逐步裁剪成小词表。裁剪规则是根据<strong>Unigram语言模型</strong>的打分依次裁剪重要度相对较低的词。</p>
<p>下面进行具体手工实现。</p>
<p>假定训练的语料(已归一化处理)为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">corpus = [</span><br><span class="line">    &quot;This is the Hugging Face Course.&quot;,</span><br><span class="line">    &quot;This chapter is about tokenization.&quot;,</span><br><span class="line">    &quot;This section shows several tokenizer algorithms.&quot;,</span><br><span class="line">    &quot;Hopefully, you will be able to understand how they are trained and generate tokens.&quot;,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>首先进行预切分处理。这里采用xlnet的预切分逻辑。具体会按照空格进行切分，标点不会切分。并且空格会保留成特殊字符”▁”，句子开头也会添加特殊字符”▁”。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from transformers import AutoTokenizer</span><br><span class="line"></span><br><span class="line"># init pre tokenize function</span><br><span class="line">xlnet_tokenizer = AutoTokenizer.from_pretrained(&quot;xlnet-base-cased&quot;)</span><br><span class="line">pre_tokenize_function = xlnet_tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str</span><br><span class="line"></span><br><span class="line"># pre tokenize</span><br><span class="line">pre_tokenized_corpus = [pre_tokenize_str(text) for text in corpus]</span><br></pre></td></tr></table></figure>

<p>获得的pre_tokenized_corpus如下，每个单元分别为[word, (start_index, end_index)]</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    [(&#x27;▁This&#x27;, (0, 4)), (&#x27;▁is&#x27;, (5, 7)), (&#x27;▁the&#x27;, (8, 11)), (&#x27;▁Hugging&#x27;, (12, 19)), (&#x27;▁Face&#x27;, (20, 24)), (&#x27;▁Course.&#x27;, (25, 32))], </span><br><span class="line">    [(&#x27;▁This&#x27;, (0, 4)), (&#x27;▁chapter&#x27;, (5, 12)), (&#x27;▁is&#x27;, (13, 15)), (&#x27;▁about&#x27;, (16, 21)), (&#x27;▁tokenization.&#x27;, (22, 35))], </span><br><span class="line">    [(&#x27;▁This&#x27;, (0, 4)), (&#x27;▁section&#x27;, (5, 12)), (&#x27;▁shows&#x27;, (13, 18)), (&#x27;▁several&#x27;, (19, 26)), (&#x27;▁tokenizer&#x27;, (27, 36)), (&#x27;▁algorithms.&#x27;, (37, 48))], </span><br><span class="line">    [(&#x27;▁Hopefully,&#x27;, (0, 10)), (&#x27;▁you&#x27;, (11, 14)), (&#x27;▁will&#x27;, (15, 19)), (&#x27;▁be&#x27;, (20, 22)), (&#x27;▁able&#x27;, (23, 27)), (&#x27;▁to&#x27;, (28, 30)), (&#x27;▁understand&#x27;, (31, 41)), (&#x27;▁how&#x27;, (42, 45)), (&#x27;▁they&#x27;, (46, 50)), (&#x27;▁are&#x27;, (51, 54)), (&#x27;▁trained&#x27;, (55, 62)), (&#x27;▁and&#x27;, (63, 66)), (&#x27;▁generate&#x27;, (67, 75)), (&#x27;▁tokens.&#x27;, (76, 83))]</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>进一步统计词频</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">word2count = defaultdict(int)</span><br><span class="line">for split_text in pre_tokenized_corpus:</span><br><span class="line">    for word, _ in split_text:</span><br><span class="line">        word2count[word] += 1</span><br></pre></td></tr></table></figure>

<p>获得word2count如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">defaultdict(&lt;class &#x27;int&#x27;&gt;, &#123;&#x27;▁This&#x27;: 3, &#x27;▁is&#x27;: 2, &#x27;▁the&#x27;: 1, &#x27;▁Hugging&#x27;: 1, &#x27;▁Face&#x27;: 1, &#x27;▁Course.&#x27;: 1, &#x27;▁chapter&#x27;: 1, &#x27;▁about&#x27;: 1, &#x27;▁tokenization.&#x27;: 1, &#x27;▁section&#x27;: 1, &#x27;▁shows&#x27;: 1, &#x27;▁several&#x27;: 1, &#x27;▁tokenizer&#x27;: 1, &#x27;▁algorithms.&#x27;: 1, &#x27;▁Hopefully,&#x27;: 1, &#x27;▁you&#x27;: 1, &#x27;▁will&#x27;: 1, &#x27;▁be&#x27;: 1, &#x27;▁able&#x27;: 1, &#x27;▁to&#x27;: 1, &#x27;▁understand&#x27;: 1, &#x27;▁how&#x27;: 1, &#x27;▁they&#x27;: 1, &#x27;▁are&#x27;: 1, &#x27;▁trained&#x27;: 1, &#x27;▁and&#x27;: 1, &#x27;▁generate&#x27;: 1, &#x27;▁tokens.&#x27;: 1&#125;)</span><br></pre></td></tr></table></figure>

<p>统计词表的全部子词和词频，取前300个词，构成最初的大词表。为了避免OOV，char级别的词均需要保留。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">char2count = defaultdict(int)</span><br><span class="line">sub_word2count = defaultdict(int)</span><br><span class="line">for word, count in word2count.items():</span><br><span class="line">    for i in range(len(word)):</span><br><span class="line">        char2count[word[i]] += count</span><br><span class="line">        for j in range(i + 2, len(word) + 1):</span><br><span class="line">            sub_word2count[word[i:j]] += count</span><br><span class="line">sorted_sub_words = sorted(sub_word2count.items(), key=lambda x: x[1], reverse=True)</span><br><span class="line"># init a large vocab with 300</span><br><span class="line">tokens = list(char2count.items()) + sorted_sub_words[: 300 - len(char2count)]</span><br></pre></td></tr></table></figure>

<p>获得的初始小词表vocabs如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&#x27;▁&#x27;, 31), (&#x27;T&#x27;, 3), (&#x27;h&#x27;, 9), (&#x27;i&#x27;, 13), (&#x27;s&#x27;, 13), ...,  (&#x27;several&#x27;, 1)]</span><br></pre></td></tr></table></figure>

<p>进一步统计每个子词的概率，并转换成Unigram里的loss贡献</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">token2count = &#123;token: count for token, count in tokens&#125;</span><br><span class="line">total_count = sum([count for token, count in token2count.items()])</span><br><span class="line">model = &#123;token: -log(count / total_count) for token, count in token2count.items()&#125;</span><br><span class="line"></span><br><span class="line">model = &#123;</span><br><span class="line">    &#x27;▁&#x27;: 2.952892114877499, </span><br><span class="line">    &#x27;T&#x27;: 5.288267030694535, </span><br><span class="line">    &#x27;h&#x27;: 4.189654742026425, </span><br><span class="line">    ..., </span><br><span class="line">    &#x27;sever&#x27;: 6.386879319362645, </span><br><span class="line">    &#x27;severa&#x27;: 6.386879319362645, </span><br><span class="line">    &#x27;several&#x27;: 6.386879319362645</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>基于每个子词的loss以及Viterbi算法就可以求解出，输入的一个词的最佳分词路径。即整体语言模型的loss最小。词的长度为N，解码的时间复杂度为O(N^2)。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">def _encode_word(word, model):</span><br><span class="line">    best_segmentations = [&#123;&quot;start&quot;: 0, &quot;score&quot;: 1&#125;] + [&#123;&quot;start&quot;: None, &quot;score&quot;: None&#125; for _ in range(len(word))]</span><br><span class="line">    for start_idx in range(len(word)):</span><br><span class="line">        # This should be properly filled by the previous steps of the loop</span><br><span class="line">        best_score_at_start = best_segmentations[start_idx][&quot;score&quot;]</span><br><span class="line">        for end_idx in range(start_idx + 1, len(word) + 1):</span><br><span class="line">            token = word[start_idx:end_idx]</span><br><span class="line">            if token in model and best_score_at_start is not None:</span><br><span class="line">                score = model[token] + best_score_at_start</span><br><span class="line">                # If we have found a better segmentation (lower score) ending at end_idx</span><br><span class="line">                if (</span><br><span class="line">                        best_segmentations[end_idx][&quot;score&quot;] is None</span><br><span class="line">                        or best_segmentations[end_idx][&quot;score&quot;] &gt; score</span><br><span class="line">                ):</span><br><span class="line">                    best_segmentations[end_idx] = &#123;&quot;start&quot;: start_idx, &quot;score&quot;: score&#125;</span><br><span class="line">    segmentation = best_segmentations[-1]</span><br><span class="line">    if segmentation[&quot;score&quot;] is None:</span><br><span class="line">        # We did not find a tokenization of the word -&gt; unknown</span><br><span class="line">        return [&quot;&lt;unk&gt;&quot;], None</span><br><span class="line">    score = segmentation[&quot;score&quot;]</span><br><span class="line">    start = segmentation[&quot;start&quot;]</span><br><span class="line">    end = len(word)</span><br><span class="line">    tokens = []</span><br><span class="line">    while start != 0:</span><br><span class="line">        tokens.insert(0, word[start:end])</span><br><span class="line">        next_start = best_segmentations[start][&quot;start&quot;]</span><br><span class="line">        end = start</span><br><span class="line">        start = next_start</span><br><span class="line">    tokens.insert(0, word[start:end])</span><br><span class="line">    return tokens, score</span><br></pre></td></tr></table></figure>

<p>例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; tokenize(&quot;This&quot;)</span><br><span class="line">&gt;&gt;&gt; ([&#x27;This&#x27;], 6.288267030694535)</span><br><span class="line">&gt;&gt;&gt; tokenize(&quot;this&quot;)</span><br><span class="line">&gt;&gt;&gt;([&#x27;t&#x27;, &#x27;his&#x27;], 10.03608902044192)</span><br></pre></td></tr></table></figure>

<p>基于上述的函数，可以获得任一个词的分词路径，以及loss。这样就可以计算整个语料上的loss。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def _compute_loss(self, model, word2count):</span><br><span class="line">    loss = 0</span><br><span class="line">    for word, freq in word2count.items():</span><br><span class="line">        _, word_loss = self._encode_word(word, model)</span><br><span class="line">        loss += freq * word_loss</span><br><span class="line">    return loss</span><br></pre></td></tr></table></figure>

<p>尝试移除model中的一个子词，并计算移除后新的model在全部语料上的loss，从而获得这个子词的score，即删除这个子词使得loss新增的量。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def _compute_scores(self, model, word2count):</span><br><span class="line">    scores = &#123;&#125;</span><br><span class="line">    model_loss = self._compute_loss(model, word2count)</span><br><span class="line">    for token, score in model.items():</span><br><span class="line">        # We always keep tokens of length 1</span><br><span class="line">        if len(token) == 1:</span><br><span class="line">            continue</span><br><span class="line">        model_without_token = copy.deepcopy(model)</span><br><span class="line">        _ = model_without_token.pop(token)</span><br><span class="line">        scores[token] = self._compute_loss(model_without_token, word2count) - model_loss</span><br><span class="line">    return scores</span><br><span class="line"></span><br><span class="line">scores = self._compute_scores(model, word2count)</span><br></pre></td></tr></table></figure>

<p>为了提升迭代效率，批量删除前10%的结果，即让整体loss增量最小的前10%的词。(删除这些词对整体loss的影响不大。)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sorted_scores = sorted(scores.items(), key=lambda x: x[1])</span><br><span class="line"># Remove percent_to_remove tokens with the lowest scores.</span><br><span class="line">for i in range(int(len(model) * 0.1)):</span><br><span class="line">    _ = token2count.pop(sorted_scores[i][0])</span><br></pre></td></tr></table></figure>

<p>获得新的词表后，重新计算每个词的概率，获得新的模型。并重复以上步骤，直到裁剪到词表大小符合要求。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">while len(model) &gt; vocab_size:</span><br><span class="line">    scores = self._compute_scores(model, word2count)</span><br><span class="line">    sorted_scores = sorted(scores.items(), key=lambda x: x[1])</span><br><span class="line">    # Remove percent_to_remove tokens with the lowest scores.</span><br><span class="line">    for i in range(int(len(model) * percent_to_remove)):</span><br><span class="line">        _ = token2count.pop(sorted_scores[i][0])</span><br><span class="line">    total_count = sum([freq for token, freq in token2count.items()])</span><br><span class="line">    model = &#123;token: -log(count / total_count) for token, count in token2count.items()&#125;</span><br></pre></td></tr></table></figure>

<p>假定预设的词表的大小为100，经过上述迭代后我们获得词表如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model = &#123;</span><br><span class="line">    &#x27;▁&#x27;: 2.318585434340487, </span><br><span class="line">    &#x27;T&#x27;: 4.653960350157523, </span><br><span class="line">    &#x27;h&#x27;: 3.5553480614894135, </span><br><span class="line">    &#x27;i&#x27;: 3.1876232813640963, </span><br><span class="line">    ...</span><br><span class="line">    &#x27;seve&#x27;: 5.752572638825633, </span><br><span class="line">    &#x27;sever&#x27;: 5.752572638825633, </span><br><span class="line">    &#x27;severa&#x27;: 5.752572638825633, </span><br><span class="line">    &#x27;several&#x27;: 5.752572638825633</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="3-2-推理阶段"><a href="#3-2-推理阶段" class="headerlink" title="3.2. 推理阶段"></a>3.2. 推理阶段</h4><p>在推理阶段，给定一个句子，需要将其切分成一个token的序列。 具体实现上先对句子进行预分词，然后对每个词基于Viterbi算法进行解码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def tokenize(self, text):</span><br><span class="line">    words = [word for word, _ in self.pre_tokenize_str(text)]</span><br><span class="line">    encoded_words = [self._encode_word(word, self.model)[0] for word in words]</span><br><span class="line">    return sum(encoded_words, [])</span><br></pre></td></tr></table></figure>

<p>例如</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; tokenize(&quot;This is the Hugging Face course!&quot;)</span><br><span class="line">&gt;&gt;&gt; [&#x27;▁This&#x27;, &#x27;▁is&#x27;, &#x27;▁the&#x27;, &#x27;▁Hugging&#x27;, &#x27;▁Face&#x27;, &#x27;▁&#x27;, &#x27;c&#x27;, &#x27;ou&#x27;, &#x27;r&#x27;, &#x27;s&#x27;, &#x27;e&#x27;, &#x27;.&#x27;]</span><br></pre></td></tr></table></figure>

<p>基于Viterbi的切分获得的是最佳切分，基于unigram可以实现一个句子的多种切分方式，并且可以获得每种切分路径的打分。</p>
<h3 id="4-SentencePiece"><a href="#4-SentencePiece" class="headerlink" title="4. SentencePiece"></a>4. SentencePiece</h3><p>SentencePiece是Google出的一个分词工具，是一种基于 BPE 的分词工具，但它与 BPE 有所不同。它直接对原始文本进行处理，不需要预先进行空格分隔等预处理，并且可以生成子词单位。SentencePiece 将文本转换为unicode码点序列，然后对码点序列应用 BPE 算法，还可以对罕见码点进行 utf-8 编码转换:</p>
<ol>
<li>文本预处理：将文本转换为unicode码点序列。</li>
<li>BPE 训练：使用 BPE 算法对码点序列进行分词训练，生成子词单元。</li>
<li>罕见码点处理：对于低频码点，可以选择保留或进行 utf-8 编码转换。</li>
<li>词汇表生成：根据训练结果生成包含子词单元的词汇表。</li>
</ol>
<ul>
<li>内置BPE，Unigram，char和word的分词方法</li>
<li>无需预分词，以unicode方式直接编码整个句子，空格会被特殊编码为▁</li>
<li>相比传统实现进行优化，分词速度速度更快</li>
</ul>
<p>当前主流的大模型都是基于sentencepiece实现，例如ChatGLM的tokenizer。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">class TextTokenizer:</span><br><span class="line">    def __init__(self, model_path):</span><br><span class="line">        self.sp = spm.SentencePieceProcessor()</span><br><span class="line">        self.sp.Load(model_path)</span><br><span class="line">        self.num_tokens = self.sp.vocab_size()</span><br><span class="line"></span><br><span class="line">    def encode(self, text):</span><br><span class="line">        return self.sp.EncodeAsIds(text)</span><br><span class="line"></span><br><span class="line">    def decode(self, ids: List[int]):</span><br><span class="line">        return self.sp.DecodeIds(ids)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>



<h2 id="三、对比分析"><a href="#三、对比分析" class="headerlink" title="三、对比分析"></a>三、对比分析</h2><table>
<thead>
<tr>
<th>分词方法</th>
<th>特点</th>
<th>优势</th>
<th>应用场景</th>
</tr>
</thead>
<tbody><tr>
<td>BPE</td>
<td>基于字节对频率合并，简单高效</td>
<td>平衡词汇表大小和文本粒度，处理罕见词效果好</td>
<td>GPT-2 等模型，多种自然语言处理任务</td>
</tr>
<tr>
<td>WordPiece</td>
<td>基于语言模型似然估计合并，考虑语义信息</td>
<td>更好地处理长尾词汇，提升模型泛化能力</td>
<td>BERT、多语言模型</td>
</tr>
<tr>
<td>SentencePiece</td>
<td>基于 BPE，直接处理原始文本，支持多种语言</td>
<td>处理无空格语言能力强，适用于多语言任务</td>
<td>中文、日语等语言处理，跨语言迁移任务</td>
</tr>
<tr>
<td>unigram</td>
<td>基于 unigram 语言模型概率，动态调整词汇表</td>
<td>提高语言模型准确性，适应文本统计特性</td>
<td>语言模型训练、语音识别等</td>
</tr>
</tbody></table>
<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/651430181">大模型基础组件 - Tokenizer</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/llm/" rel="tag"># llm</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/08/27/2023-09-09-Linux-2-Awk%E5%91%BD%E4%BB%A4%E5%8F%8A%E7%A4%BA%E4%BE%8B/" rel="prev" title="[2023-09-09] Linux #2:Awk命令及示例">
      <i class="fa fa-chevron-left"></i> [2023-09-09] Linux #2:Awk命令及示例
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#LLM%E4%B9%8B%E5%88%86%E8%AF%8D%E5%99%A8-tokenizer"><span class="nav-number">1.</span> <span class="nav-text">LLM之分词器(tokenizer)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E5%BC%95%E8%A8%80"><span class="nav-number">1.1.</span> <span class="nav-text">一、引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E6%96%B9%E6%B3%95%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.2.</span> <span class="nav-text">二、方法介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-BPE"><span class="nav-number">1.2.1.</span> <span class="nav-text">1. BPE</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">1.1. 训练阶段</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-%E6%8E%A8%E7%90%86%E9%98%B6%E6%AE%B5"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">1.2. 推理阶段</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-WordPiece"><span class="nav-number">1.2.2.</span> <span class="nav-text">2. WordPiece</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">2.1. 训练阶段</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-%E6%8E%A8%E7%90%86%E9%98%B6%E6%AE%B5"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">2.2. 推理阶段</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Unigram"><span class="nav-number">1.2.3.</span> <span class="nav-text">3. Unigram</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">3.1. 训练阶段</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-%E6%8E%A8%E7%90%86%E9%98%B6%E6%AE%B5"><span class="nav-number">1.2.3.2.</span> <span class="nav-text">3.2. 推理阶段</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-SentencePiece"><span class="nav-number">1.2.4.</span> <span class="nav-text">4. SentencePiece</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90"><span class="nav-number">1.3.</span> <span class="nav-text">三、对比分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%95%E7%94%A8"><span class="nav-number">1.4.</span> <span class="nav-text">引用</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="TheChef"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">TheChef</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">10</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">TheChef</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="eye"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
